"""
çŸ¥è¯†å¯¼å›¾/æçº²ç”Ÿæˆæ¨¡å—
è´Ÿè´£ç”Ÿæˆç« èŠ‚çš„çŸ¥è¯†ç»“æ„å’Œå­¦ä¹ é‡ç‚¹
"""

from typing import Dict, Optional

from zhipuai import ZhipuAI

from config import ZHIPUAI_API_KEY, LLM_MODEL, KNOWLEDGE_MAP_PROMPT


class KnowledgeMapGenerator:
    """çŸ¥è¯†å¯¼å›¾ç”Ÿæˆå™¨"""
    
    def __init__(self, rag_engine=None):
        self.rag_engine = rag_engine
        self.zhipu_client = None
        self.cached_map = None
        self._init_zhipu()
    
    def _init_zhipu(self):
        """åˆå§‹åŒ–æ™ºè°±AIå®¢æˆ·ç«¯"""
        if ZHIPUAI_API_KEY:
            self.zhipu_client = ZhipuAI(api_key=ZHIPUAI_API_KEY)
    
    def generate_outline(self, content: Optional[str] = None) -> str:
        """
        ç”ŸæˆçŸ¥è¯†æçº²
        content: ç« èŠ‚å†…å®¹ï¼Œå¦‚æœä¸ºNoneåˆ™ä»çŸ¥è¯†åº“è·å–
        """
        if not self.zhipu_client:
            return "âŒ æ™ºè°±AIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·è®¾ç½®API Key"
        
        # è·å–å†…å®¹
        if content is None:
            if self.rag_engine:
                # ä»çŸ¥è¯†åº“è·å–æ‰€æœ‰æ–‡æ¡£çš„æ‘˜è¦
                docs = self.rag_engine.search("å¼‚å¸¸æ£€æµ‹çš„ä¸»è¦å†…å®¹å’Œæ–¹æ³•", top_k=10)
                if docs:
                    content = "\n\n".join([doc["text"] for doc in docs])
                else:
                    content = self._get_default_content()
            else:
                content = self._get_default_content()
        
        prompt = KNOWLEDGE_MAP_PROMPT + content
        
        try:
            response = self.zhipu_client.chat.completions.create(
                model=LLM_MODEL,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®æŒ–æ˜æ•™å­¦ä¸“å®¶ï¼Œæ“…é•¿æ•´ç†å’Œç»„ç»‡çŸ¥è¯†ç‚¹ã€‚è¯·ç”Ÿæˆæ¸…æ™°ã€ç»“æ„åŒ–çš„çŸ¥è¯†æçº²ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.5,
                max_tokens=2000
            )
            
            outline = response.choices[0].message.content
            self.cached_map = outline
            return outline
        
        except Exception as e:
            return f"âŒ ç”ŸæˆçŸ¥è¯†æçº²å¤±è´¥: {e}"
    
    def generate_key_concepts(self) -> str:
        """ç”Ÿæˆæ ¸å¿ƒæ¦‚å¿µåˆ—è¡¨"""
        if not self.zhipu_client:
            return "âŒ æ™ºè°±AIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–"
        
        prompt = """è¯·åˆ—å‡ºã€Šæ•°æ®æŒ–æ˜å¯¼è®ºã€‹ç¬¬10ç« "å¼‚å¸¸æ£€æµ‹"ä¸­çš„æ ¸å¿ƒæ¦‚å¿µã€‚

è¦æ±‚ï¼š
1. æ¯ä¸ªæ¦‚å¿µç”¨ä¸€å¥è¯ç®€è¦è§£é‡Š
2. æ ‡æ³¨éš¾åº¦çº§åˆ«ï¼ˆåŸºç¡€/ä¸­ç­‰/é«˜çº§ï¼‰
3. æŒ‰é‡è¦ç¨‹åº¦æ’åº

æ ¼å¼ï¼š
ğŸ”¹ **æ¦‚å¿µåç§°** [éš¾åº¦] - ç®€è¦è§£é‡Š
"""
        
        try:
            response = self.zhipu_client.chat.completions.create(
                model=LLM_MODEL,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.5,
                max_tokens=1500
            )
            
            return response.choices[0].message.content
        
        except Exception as e:
            return f"âŒ ç”Ÿæˆæ¦‚å¿µåˆ—è¡¨å¤±è´¥: {e}"
    
    def generate_learning_path(self) -> str:
        """ç”Ÿæˆå­¦ä¹ è·¯å¾„å»ºè®®"""
        if not self.zhipu_client:
            return "âŒ æ™ºè°±AIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–"
        
        prompt = """ä¸ºå­¦ä¹ ã€Šæ•°æ®æŒ–æ˜å¯¼è®ºã€‹ç¬¬10ç« "å¼‚å¸¸æ£€æµ‹"è®¾è®¡ä¸€ä¸ªå­¦ä¹ è·¯å¾„ã€‚

è¦æ±‚ï¼š
1. åˆ†ä¸ºè‹¥å¹²å­¦ä¹ é˜¶æ®µ
2. æ¯ä¸ªé˜¶æ®µæ ‡æ³¨é¢„è®¡å­¦ä¹ æ—¶é—´
3. åŒ…å«å­¦ä¹ ç›®æ ‡å’Œæ£€éªŒæ–¹æ³•
4. æä¾›å­¦ä¹ å»ºè®®

æ ¼å¼ï¼š
## ğŸ“š å¼‚å¸¸æ£€æµ‹å­¦ä¹ è·¯å¾„

### é˜¶æ®µ1: xxx (é¢„è®¡xxåˆ†é’Ÿ)
- **ç›®æ ‡**: ...
- **å†…å®¹**: ...
- **æ£€éªŒ**: ...
- **å»ºè®®**: ...
"""
        
        try:
            response = self.zhipu_client.chat.completions.create(
                model=LLM_MODEL,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.6,
                max_tokens=1500
            )
            
            return response.choices[0].message.content
        
        except Exception as e:
            return f"âŒ ç”Ÿæˆå­¦ä¹ è·¯å¾„å¤±è´¥: {e}"
    
    def _get_default_content(self) -> str:
        """è·å–é»˜è®¤çš„å¼‚å¸¸æ£€æµ‹çŸ¥è¯†å†…å®¹"""
        return """
ç¬¬10ç«  å¼‚å¸¸æ£€æµ‹ (Anomaly Detection)

10.1 å¼‚å¸¸æ£€æµ‹æ¦‚è¿°
- å¼‚å¸¸çš„å®šä¹‰ï¼šä¸å¤§å¤šæ•°æ•°æ®æ˜¾è‘—ä¸åŒçš„æ•°æ®å¯¹è±¡
- å¼‚å¸¸çš„åˆ«åï¼šç¦»ç¾¤ç‚¹(outlier)ã€å¼‚å¸¸å€¼(anomaly)ã€ä¾‹å¤–(exception)
- åº”ç”¨åœºæ™¯ï¼šæ¬ºè¯ˆæ£€æµ‹ã€å…¥ä¾µæ£€æµ‹ã€åŒ»å­¦è¯Šæ–­ã€æ•…éšœæ£€æµ‹

10.2 å¼‚å¸¸çš„ç±»å‹
- å…¨å±€å¼‚å¸¸(Global Outlier)ï¼šç›¸å¯¹äºæ•´ä¸ªæ•°æ®é›†çš„å¼‚å¸¸
- æƒ…å¢ƒå¼‚å¸¸(Contextual Outlier)ï¼šåœ¨ç‰¹å®šæƒ…å¢ƒä¸­çš„å¼‚å¸¸
- é›†ä½“å¼‚å¸¸(Collective Outlier)ï¼šä¸€ç»„æ•°æ®ç‚¹å…±åŒè¡¨ç°å¼‚å¸¸

10.3 å¼‚å¸¸æ£€æµ‹æ–¹æ³•
10.3.1 ç»Ÿè®¡æ–¹æ³•
- å‚æ•°æ–¹æ³•ï¼šå‡è®¾æ•°æ®æœä»æŸç§åˆ†å¸ƒ
- éå‚æ•°æ–¹æ³•ï¼šä¸å‡è®¾ç‰¹å®šåˆ†å¸ƒ

10.3.2 åŸºäºé‚»è¿‘åº¦çš„æ–¹æ³•
- åŸºäºè·ç¦»ï¼škè¿‘é‚»è·ç¦»
- åŸºäºå¯†åº¦ï¼šå±€éƒ¨ç¦»ç¾¤å› å­(LOF)

10.3.3 åŸºäºèšç±»çš„æ–¹æ³•
- å°†ä¸å±äºä»»ä½•ç°‡çš„ç‚¹è§†ä¸ºå¼‚å¸¸
- ä¸ç°‡ä¸­å¿ƒè·ç¦»è¿‡å¤§çš„ç‚¹è§†ä¸ºå¼‚å¸¸

10.4 è¯„ä¼°æ–¹æ³•
- å‡†ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
- ROCæ›²çº¿ã€AUC
"""
    
    def generate_summary(self, topic: str = "å¼‚å¸¸æ£€æµ‹") -> str:
        """ç”ŸæˆæŒ‡å®šä¸»é¢˜çš„æ‘˜è¦"""
        if not self.zhipu_client:
            return "âŒ æ™ºè°±AIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–"
        
        # ä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³å†…å®¹
        if self.rag_engine:
            docs = self.rag_engine.search(topic, top_k=5)
            if docs:
                content = "\n\n".join([doc["text"] for doc in docs])
            else:
                content = self._get_default_content()
        else:
            content = self._get_default_content()
        
        prompt = f"""åŸºäºä»¥ä¸‹å†…å®¹ï¼Œç”Ÿæˆå…³äº"{topic}"çš„ç®€æ˜æ‘˜è¦ã€‚

è¦æ±‚ï¼š
1. æ¦‚æ‹¬æ ¸å¿ƒè¦ç‚¹
2. è¯­è¨€ç®€æ´æ¸…æ™°
3. æ§åˆ¶åœ¨200å­—ä»¥å†…

å†…å®¹ï¼š
{content}
"""
        
        try:
            response = self.zhipu_client.chat.completions.create(
                model=LLM_MODEL,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.5,
                max_tokens=500
            )
            
            return response.choices[0].message.content
        
        except Exception as e:
            return f"âŒ ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}"


def test_knowledge_map():
    """æµ‹è¯•çŸ¥è¯†å¯¼å›¾ç”Ÿæˆ"""
    generator = KnowledgeMapGenerator()
    
    print("=" * 50)
    print("ç”ŸæˆçŸ¥è¯†æçº²...")
    print("=" * 50)
    outline = generator.generate_outline()
    print(outline)
    
    print("\n" + "=" * 50)
    print("ç”Ÿæˆæ ¸å¿ƒæ¦‚å¿µ...")
    print("=" * 50)
    concepts = generator.generate_key_concepts()
    print(concepts)


if __name__ == "__main__":
    test_knowledge_map()

